![[Pasted image 20250110225333.png]]
### Ejercicio 1
##### a) Para etiquetar SOLO usa DATE si no te especifican nada mas. Igualmente se le puede hablar sobre orden total o parcial teniendo las etiquetas de solo DATE, solo habría que añadir el PID a la derecha, es algo irrelevante. Pero que en una pregunta de teoría sobre los ordenes se lo puedes indicar.
![[Pasted image 20250110230258.png]]
##### b)
![[Pasted image 20250110231802.png]]

##### c)
Falso, los relojes vectoriales determinan orden parcial. La ventaja que tienen sobre los escalares (los cuales además pueden determinar orden total mediante (date, PID)), es que los vectoriales pueden determinar si los eventos están causalmente relacionados, es decir. Distinguen entre eventos concurrentes. Por ejemplo (1,0,0,0) y (0,1,0,0) no son comparables al no tener una relación de causalidad.

![[Pasted image 20250111002114.png]]
![[Pasted image 20250111002133.png]]
![[Pasted image 20250111002202.png]]
![[Pasted image 20250111002217.png]]

### Ejercicio 2
##### a)
`when sending a REQUEST (acquire_mutex) do`
	`lrdi <- clocki + 1`
	`send(REQUEST(lrdi,i))
`when receiving REQUEST(lrdi,i)`
	`clockj <- max(clockj,lrdi)`

Esto sin tener en cuenta el aumento de reloj en eventos internos. Es decir solo utilizamos el reloj para los mutex. Y teniendo en cuenta que clock = Highest_Sequence_Number, y lrd = Our_Sequence_Number.
##### b)
En los relojes lógicos de Lamport, el enfoque principal es mantener un orden causal entre eventos en un sistema distribuido. Esto se logra mediante un reloj lógico local que se incrementa en eventos internos y se sincroniza con los mensajes enviados y recibidos. Este mecanismo no está diseñado para gestionar acceso a secciones críticas.

Por otro lado, en el algoritmo de Ricart-Agrawala, los relojes tienen una función específica para coordinar el acceso a una sección crítica. Se utilizan dos relojes:
1. **`Our Sequence`**: Representa el número de secuencia generado al solicitar acceso a la sección crítica.
2. **`Highest Sequence`**: Almacena el valor más alto de secuencia observado en cualquier mensaje recibido.
Estas diferencias se deben a los objetivos de cada sistema:
- Los relojes de Lamport buscan establecer un orden causal entre eventos en general.
- Los relojes en Ricart-Agrawala están diseñados específicamente para garantizar exclusión mutua en la sección crítica, asegurando que las solicitudes se procesen en el orden correcto y gestionando las prioridades de acceso.

Además, en Ricart-Agrawala no se incrementa el reloj en eventos internos porque no son relevantes para la gestión de la sección crítica, mientras que en Lamport sí se considera esencial para mantener el orden causal entre todos los eventos del sistema.

En Ricart-Agrawala, el reloj solo se incrementa cuando un nodo **envía una solicitud** de acceso a la sección crítica. Este incremento asegura que las solicitudes tengan un ordenado, ya que en caso de que fuesen iguales, se desempata con el PID del proceso.

##### c)
P1,P2,P3 y P4
estado inicial:
	P1 en SC
	P2,P3 y P4 fuera de SC y no han solicitado mutex
###### c1)
1. P3 solicita mutex
2. P2 solicita mutex
3. P4 solicita mutex

Suponiendo que llegan en ese orden y que ha dado tiempo que los relojes se sincronicen correctamente:

**P3 solicita mutex**
Estado P1:
"dentro de SC"
`Reply_Deferred[FALSE,FALSE,TRUE,FALSE]`
Estado P2:
"en otras tareas"
`Reply_Deferred[FALSE,FALSE,FALSE,FALSE]`
Estado P3:
"solicitando mutex"
`Reply_Deferred[FALSE,FALSE,FALSE,FALSE]`
Estado P4:
"en otras tareas"
`Reply_Deferred[FALSE,FALSE,FALSE,FALSE]`

**P2 solicita mutex**
Estado P1:
"dentro de SC"
`Reply_Deferred[FALSE,TRUE,TRUE,FALSE]`
Estado P2:
"solicitando mutex"
`Reply_Deferred[FALSE,FALSE,FALSE,FALSE]`
Estado P3:
"solicitando mutex"
`Reply_Deferred[FALSE,TRUE,FALSE,FALSE]`
Estado P4:
"en otras tareas"
`Reply_Deferred[FALSE,FALSE,FALSE,FALSE]`

**P4 solicita mutex**
Estado P1:
"dentro de SC"
`Reply_Deferred[FALSE,TRUE,TRUE,TRUE]`
Estado P2:
"solicitando mutex"
`Reply_Deferred[FALSE,FALSE,FALSE,TRUE]`
Estado P3:
"solicitando mutex"
`Reply_Deferred[FALSE,TRUE,FALSE,TRUE]`
Estado P4:
"solicitando mutex"
`Reply_Deferred[FALSE,FALSE,FALSE,FALSE]`

Si fuese todo a la vez habría que desempatar por PID

###### c2)
**P4 solicita mutex**
Estado P1:
"en otras tareas"
`Reply_Deferred[FALSE,FALSE,FALSE,FALSE]`
Estado P2:
"solicitando mutex"
`Reply_Deferred[FALSE,FALSE,FALSE,TRUE]`
Estado P3:
"dentro de SC"
`Reply_Deferred[FALSE,TRUE,FALSE,TRUE]`
Estado P4:
"solicitando mutex"
`Reply_Deferred[FALSE,FALSE,FALSE,FALSE]`

![[Pasted image 20250111164457.png]]
### Ejercicio 3
##### a)
Sirve para seleccionar un proceso que actúe como coordinador o líder en un sistema distribuido. El líder será responsable de gestionar tareas críticas, como la asignación de recursos, la sincronización o la toma de decisiones globales. Además, este mecanismo aumenta la tolerancia a fallos ya que permite al sistema recuperarse automáticamente cuando el líder falla.
##### b)

**RTT** significa **Round-Trip Time** o tiempo de ida y vuelta. Es el tiempo que tarda un mensaje en viajar desde un emisor hasta un receptor y que la respuesta de ese receptor vuelva al emisor.

- **Mejor caso (1 RTT, N-1 mensajes)**:
    - Ocurre cuando el proceso con **mayor PID detecta directamente la falla** del líder.
    - Este proceso se autoproclama líder y envía un único mensaje de **Coordinador** a los demás procesos.
- **Peor caso (N-1 RTTs, O(N²) mensajes)**:
    - Sucede si el proceso con **menor PID inicia la elección**.
    - Este proceso debe enviar mensajes de **Elección** a procesos con mayor PID, quienes a su vez continúan propagando la elección hasta llegar al proceso con el mayor PID. Luego, este último envía mensajes de **Coordinador** a todos los demás

![[Pasted image 20250111175119.png]]
##### c)
- **Timeout para detectar la falla del líder**:
    - Cada proceso monitorea al líder actual enviándole mensajes periódicos de "latido" (heartbeat) y espera su respuesta.
    - Si el proceso no recibe respuesta dentro de un tiempo definido, el timeout expira y el proceso asume que el líder ha fallado, iniciando una elección.
- **Timeout para esperar respuestas durante la elección**:
    - Cuando un proceso inicia una elección, envía mensajes de Elección a los procesos con mayor PID y espera respuestas (mensajes de Respuesta o Coordinador).
    - Si no recibe ninguna respuesta dentro del tiempo definido, el proceso asume que los procesos con mayor PID han fallado y se declara líder enviando un mensaje de Coordinador a todos los procesos vivos.
- Se ajustan con `2*RTT + Tprocesado`

![[Pasted image 20250111175250.png]]
### Ejercicio 4
##### 4.1
Un ReplicaSet en Kubernetes es un controlador que asegura que un número específico de Pods esté corriendo en todo momento, pero no incluye un mecanismo de consenso El ReplicaSet no tiene lógica de consenso como la que implementan sistemas distribuidos como Raft o Paxos En lugar de eso, simplemente asegura que los Pods especificados existan y se ejecuten en el clúster, recreándolos si fallan o son eliminados.
##### 4.2
Falso, en Raft, una vez que una entrada de registro ha sido comprometida por un líder se garantiza que esa entrada no será modificada por ningún líder futuro. Esto se debe a las propiedades de seguridad de Raft, que aseguran que:
- Las entradas comprometidas son permanentes
    - Si una entrada está replicada en la mayoría de los nodos y el líder la marca como comprometida, ningún líder futuro puede sobrescribirla.
- Condición para que un nuevo líder sea elegido
    - Un nodo solo puede convertirse en líder si tiene el log más actualizado, lo que incluye todas las entradas comprometidas por líderes anteriores.
- Log Matching Property
	- Si dos entradas de registro en diferentes servidores tienen mismo índice y mandato => almacenan la misma operación y los registros son idénticos para todas las entradas anteriores
##### 4.3
Falso, la noción de "cadena de confianza" en la gestión de certificados digitales no se basa en una cadena de servidores en los que confiamos para la transmisión de certificados. En su lugar, se refiere a un modelo jerárquico en el que se confía en una entidad raíz (la Autoridad Certificadora Raíz, o CA Raíz), la cual emite certificados a otras Autoridades Certificadoras (CA intermedias). Estas, a su vez, emiten certificados a usuarios finales o entidades. La confianza en los certificados digitales deriva de esta jerarquía, no de una cadena de servidores.
La clave aquí es que la confianza en un certificado digital depende de que cada eslabón en esta cadena de certificación sea válido y confiable, y no de servidores que transmitan los certificados.
##### 4.4
Verdadero Dos réplicas pueden tener entradas en el mismo índice con diferentes mandatos, pero esto solo ocurre si las entradas con el mismo índice y las posteriores no estaban comprometidas. Esto se debe a que Raft garantiza que una vez que una entrada está comprometida, ni esa entrada ni las posteriores pueden ser sobrescritas.

Durante la sincronización, el nuevo líder corrige estas discrepancias sobrescribiendo entradas conflictivas en los nodos seguidores para garantizar la Log Matching Property que establece que las entradas en los mismos índices deben coincidir en término y contenido en todos los nodos del clúster
##### 4.5
Verdadero, el modelo de consistencia eventual mejora el rendimiento y la disponibilidad en sistemas distribuidos al relajar las restricciones de consistencia secuencial. Permite que las réplicas procesen actualizaciones de forma asíncrona, sin garantizar una actualización inmediata en todos los nodos. Sin embargo, asegura que todas las réplicas convergerán al mismo estado eventualmente, lo que lo hace ideal para sistemas que priorizan disponibilidad y tolerancia a fallos ofreciendo menores latencias y mayor escalabilidad. No como la consistencia secuencial, que requiere sincronización global para garantizar un orden único de las operaciones, lo que incrementa la latencia y reduce el rendimiento
##### 4.6
Falso, Kubernetes no asocia un Service directamente con un ReplicaSet, Deployment o StatefulSet utilizando el nombre del recurso "Service". En su lugar, utiliza un mecanismo basado en etiquetas (labels y selectores (selectors para establecer esta asociación
##### 4.7 
Falso, aunque un algoritmo de elección de líder está relacionado con el consenso, no es un algoritmo de consenso completo. La elección de líder tiene el propósito de seleccionar un único nodo que actuará como coordinador, pero no necesariamente garantiza un acuerdo sobre valores u operaciones como lo hacen los algoritmos de consenso (por ejemplo, Paxos o Raft). El consenso implica acordar un valor común entre todos los nodos, mientras que la elección de líder se centra solo en identificar un líder.
##### 4.8
Falso, en Raft, una partición de red no permite que se comprometan entradas distintas en dos réplicas. Esto se debe a que Raft utiliza el concepto de mayoría (quórum) para comprometer entradas. Si ocurre una partición de red, solo puede haber un único grupo que alcance la mayoría (quórum) durante una partición de red, pero no dos grupos diferentes al mismo tiempo ya que la mayoría de nodos está garantizada para ser única `(N/2 - 1)`.

Por lo tanto, aunque una partición de red puede hacer que un grupo minoritario no pueda comprometer nuevas entradas debido a la falta de comunicación con el líder, no es posible que dos diferentes líderes comprometan entradas conflictivas en el mismo índice del registro. Esto se garantiza porque Raft previene la elección de múltiples líderes simultáneos mediante su mecanismo de votación y mandatos.
##### 4.9
Falso, Raft no utiliza únicamente el concepto de mandatos para determinar un nuevo líder. El proceso para elegir un nuevo líder incluye:
1. Mandatos: Cada elección ocurre en un nuevo mandato, que es un número incremental que ayuda a diferenciar los ciclos de liderazgo.
2. Quórum (mayoría de votos): El candidato necesita obtener una mayoría de votos de los nodos del clúster para convertirse en líder.
3. Reglas adicionales: Raft asegura que el nuevo líder tenga el log actualizado antes de ser elegido, lo que se verifica con el índice y el término de las entradas del registro.
##### 4.10
**Falso.**
Kerberos no utiliza la técnica de firma digital para autenticar a sus servidores. En lugar de ello, Kerberos se basa principalmente en cifrado simétrico para autenticar tanto a usuarios como a servidores. En Kerberos, el KDC (Centro de Distribución de Claves emite tickets como el Ticket Granting Ticket (TGT que el usuario recibe tras autenticarse inicialmente. Este TGT se utiliza posteriormente para solicitar Tickets de Servicio (TS al KDC. Los servidores confían en estos tickets, que contienen información cifrada con claves compartidas, para autenticar a los usuarios y garantizar la integridad de las sesiones. Por lo que no utiliza la autenticación asimétrica de las firmas.