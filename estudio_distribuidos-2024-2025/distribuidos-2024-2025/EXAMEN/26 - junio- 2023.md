![[Pasted image 20250112022426.png]]
![[Pasted image 20250112022440.png]]

### Ejercicio 1
##### a)
![[Pasted image 20250112023151.png]]
##### b)
Falso, las estampillas vectoriales de a y o nos dan un orden: a sucede antes que o porque la estampilla de a es `[1,0,0,0]` y la de o es `[2,0,4,0]`. Por lo que se puede observar perfectamente como a ocurre antes `P[1] = 1` frente a `P[1] = 2` en b. 1 < 2 => a->b
`[1,0,0,0] <= [2,0,4,0]` Por lo que a ocurre antes

Por lo tanto, la afirmación original de que "las estampillas vectoriales no nos dicen nada sobre la relación de causalidad" es **falsa**. Las estampillas vectoriales permiten observar directamente la relación de causalidad cuando existe.

##### c)
Falso, es cierto que el evento h es concurrente al evento q, y que los relojes vectoriales al dar un orden parcial no permiten comparar eventos concurrentes. Sin embargo, los relojes escalares si que permiten comparar eventos concurrentes al añadir como desempate el PID proporcionando así un orden total.

Los relojes escalares con el mecanismo de (date,PID) sí permiten generar un orden total. En este caso, si los valores escalares (date) de h y q son iguales, se utiliza el PID como desempate.

![[Pasted image 20250112160221.png]]
![[Pasted image 20250112160238.png]]
![[Pasted image 20250112160248.png]]
![[Pasted image 20250112160800.png]]

### Ejercicio 2
##### a)
Sí, para resolver el problema de exclusión múltiple distribuida (multiplex) en lugar de exclusión mutua (mutex), simplemente habría que modificar la condición del WAITFOR
En vez de esperar a que el Outstanding_Reply_Count sea igual a 0 (lo cual significa que todos los nodos han respondido y se tiene acceso exclusivo), deberías ajustar la lógica para permitir que hasta un número predefinido de procesos puedan entrar simultáneamente en la sección crítica.
Por ejemplo:
- Definir un límite L, el número máximo de procesos permitidos en la sección crítica al mismo tiempo.
- Cambiar el WAITFOR para que espere hasta que N−L respuestas hayan sido recibidas en lugar de N−1.
En términos simples, si permites que L procesos estén en la sección crítica simultáneamente, el algoritmo permitiría más concurrencia manteniendo la coordinación.
Esto implica cambios también en la lógica de prioridad y en cómo se otorgan permisos (REPLY) para mantener el control sobre cuántos procesos están activos en la sección crítica
##### b)
En el algoritmo clásico de Ricart-Agrawala (RA), si un nodo falla y no responde a las solicitudes, los demás nodos no pueden entrar en la sección crítica porque el nodo fallido no enviará su respuesta de REPLY lo que bloquea el acceso.
En el caso del algoritmo modificado para exclusión múltiple (multiplex), la tolerancia a fallos mejora significativamente porque no es necesario esperar a que todos los nodos respondan, sino solo a un número suficiente de respuestas para alcanzar el límite L de procesos permitidos en la sección crítica. Esto implica que pueden fallar hasta N−L nodos antes de que el sistema deje de funcionar, lo que aumenta la tolerancia a fallos del sistema.
##### c.1)

Verdadero, si todos los procesos tienen inicialmente sus variables `our_sequence_number` y `highest_sequence_number` igual a 0 y solicitan acceso simultáneamente, el algoritmo de Ricart-Agrawala utiliza el PID como desempate. Esto significa que el proceso con el menor PID (en este caso, P1) tendrá prioridad y accederá primero a la sección crítica.
##### c.2)
La propiedad de ausencia de inanición establece que ningún proceso que solicite la sección crítica puede ser indefinidamente bloqueado. En el algoritmo de Ricart-Agrawala:
- Por qué no hay inanición Si un proceso desea acceder a la sección crítica, eventualmente obtendrá los permisos necesarios porque:
    1. El protocolo garantiza que las solicitudes se procesan en orden de timestamp (`our_sequence_number`).
    2. Incluso si muchos procesos compiten por la sección crítica, el tiempo que cada uno tiene que esperar está acotado, ya que el algoritmo asegura un progreso en la secuencia.
Por lo tanto, no existen problemas de inanición en el algoritmo clásico, ya que todas las solicitudes son gestionadas en base al reloj lógico y PID, garantizando que cada proceso tendrá su turno.
##### c.3)
En presencia de fallos, la situación cambia respecto a la ausencia de inanición y ausencia de bloqueos (deadlocks)
- Inanición en presencia de fallos:
    - Si un nodo que debe enviar un `REPLY` a otros nodos falla, puede bloquear el progreso de esos procesos, ya que no recibirán la respuesta necesaria para continuar. Esto genera un problema de inanición.
    - Por lo tanto, la ausencia de inanición no está garantizada en presencia de fallos, ya que los nodos dependientes del nodo fallido quedarán esperando indefinidamente.
- **Deadlocks en presencia de fallos**:
    - El algoritmo de Ricart-Agrawala, en su diseño original, no produce bloqueos porque no tiene dependencias cíclicas entre procesos. Sin embargo, si un nodo falla mientras otros esperan su respuesta, se produce un bloqueo funcional (aunque técnicamente no es un deadlock cíclico).
    - Por lo tanto, no hay deadlocks clásicos, pero hay un problema relacionado con el fallo que impide que otros nodos progresen.

![[Pasted image 20250112161755.png]]
![[Pasted image 20250112161806.png]]


##### 4.1
Verdadero:
- Tener solo el certificado digital de la Autoridad Certificadora (CA) no es suficiente para validar el certificado digital de un usuario.
- Necesitas la clave pública de la CA para verificar la firma digital del certificado. Esto asegura que el certificado no ha sido alterado y proviene de una CA confiable.
- Además, necesitas comprobar otros aspectos del certificado, como su fecha de expiración y si ha sido revocado (CLR).
- Aunque en muchos casos el certificado digital de la CA ya contiene la Kpub por lo que depende de la interpretación también puede ser falso...
##### 4.2
Falso:
- Mejor caso: 1 RTT y N-1 mensajes, ocurre cuando el proceso con el mayor PID detecta directamente el fallo del líder y se proclama como nuevo líder, notificando a los demás procesos.
- Peor caso: N-1 RTTs y O(N² mensajes), ocurre cuando el proceso con el menor PID inicia la elección, propagándose las solicitudes de elección a todos los procesos con mayor PID, hasta que el proceso con el mayor PID se convierte en líder y notifica a los demás.
##### 4.3
Falso, frente a particiones de red en ambos algoritmos se crearían grupos con diferentes líderes.
Ambos algoritmos carecen de mecanismos para detectar o manejar particiones de red, lo que puede llevar a inconsistencias en los sistemas distribuidos. Por tanto, ninguno es más robusto que el otro frente a particiones de red, ya que ambos comparten la misma vulnerabilidad inherente: la posibilidad de múltiples líderes en diferentes particiones.
##### 4.4
Falso, todos conocen todos.
##### 4.5
Falso, en el algoritmo del matón existen cuatro tipos de mensajes, no solo dos:
1. **Elección**: Para anunciar el inicio de un proceso de elección.
2. **Respuesta (OK)**: Para confirmar que un proceso con un PID más alto responde a la elección.
3. **Coordinador**: Para anunciar que un nuevo líder ha sido elegido.
4. **Latido (heartbeat)**: Para verificar si el líder actual sigue funcionando.
##### 4.6 
Falso, el modelo de consistencia secuencial es más restrictivo que el de consistencia causal, por lo que no es correcto decir que el modelo causal engloba al secuencial.
- Consistencia secuencial: Garantiza que todas las operaciones se observan en el mismo orden por todos los procesos, independientemente de las relaciones de causalidad.
- Consistencia causal: Solo garantiza que las operaciones relacionadas causalmente se observan en el mismo orden, pero permite que operaciones no relacionadas se vean en distinto orden por diferentes procesos.
Dado que la consistencia secuencial asegura un orden global único para todas las operaciones, es más restrictiva y, por tanto, engloba a la consistencia causal.
##### 4.7
Falso, en Kubernetes no solo el recurso ReplicaSet proporciona tolerancia a fallos. También otros recursos como Deployment y StatefulSet incluyen mecanismos para garantizar la tolerancia a fallos.
- ReplicaSet: Mantiene un número fijo de réplicas de pods, asegurando que siempre haya el número especificado funcionando.
- Deployment: Encima de un ReplicaSet, permite gestionar actualizaciones y rollbacks, proporcionando tolerancia a fallos de manera más avanzada.
- StatefulSet: Similar a ReplicaSet, pero diseñado para aplicaciones con estado, asegurando tolerancia a fallos y orden de inicialización de pods.
##### 4.8
Falso, los mecanismos de transacción distribuida y consenso de Raft no son equivalentes. Aunque ambos se utilizan en sistemas distribuidos, tienen diferencias clave.
##### 4.9
Verdadero. Un algoritmo de exclusión mutua distribuida no es un algoritmo de consenso porque tienen objetivos diferentes:
- Exclusión mutua distribuida: Garantiza que un único proceso tenga acceso exclusivo a un recurso crítico en un momento dado, gestionando el acceso mediante protocolos de mensajes para evitar conflictos.
- Consenso: Se utiliza para que los nodos de un sistema acuerden un único valor en presencia de fallos o concurrencia, como decidir qué entrada añadir a un registro replicado o quién será el líder.
##### 4.10
Verdadero. Los mandatos de Raft sirven para dividir el tiempo en épocas de liderazgo, identificando el líder actual en un período específico. No tienen relación con los relojes lógicos porque no reflejan causalidad entre eventos ni establecen un orden parcial como lo hacen los relojes lógicos. Su función principal es coordinar y sincronizar los nodos en relación con el liderazgo, no la secuencia de eventos.
##### 4.11 
Falso. Kerberos no distribuye certificados digitales basados en criptografía de clave pública/privada. En su lugar, utiliza un sistema basado en tickets. El servidor de Kerberos emite un Ticket Granting Ticket (TGT) tras autenticar al usuario, que luego se utiliza para obtener un Service Ticket (TS) para acceder a servicios específicos. Este enfoque se basa en claves simétricas compartidas, no en infraestructura de clave pública (PKI).

##### 4.12
Falso. El modelo de máquina de estados replicada, como el utilizado en Raft, no solo opera sobre "datos de datos" sino que replica operaciones. Estas operaciones se registran en un log replicado y luego se aplican de forma determinista a las máquinas de estados de todos los nodos. Esto garantiza que todos los nodos mantengan un estado consistente, incluso en caso de fallos. Raft es un claro ejemplo que contradice la afirmación, ya que se centra precisamente en replicar operaciones.